{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "# display the plot in a separate window\n",
    "import matplotlib\n",
    "\n",
    "# NOTE !!!!!!!!!\n",
    "matplotlib.use('Agg')\n",
    "# NOTE since I run on the remote server (which is a headless environment that doesn't support tk iterative framework)\n",
    "# NOTE so I use agg (non-interactive backend) and save related png in the work folder \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# NOTE since I run on the remote server (where it is hard to render this picture in a new window), \n",
    "# NOTE so I have just savepng to achieve the same requirement\n",
    "fig.savefig(\"blank.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since I run on the remote server (which is a headless environment that doesn't support tk iterative framework) \\\n",
    "so I use agg (non-interactive backend) and save related png in the work folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test,y_test)\n",
    "\n",
    "# NOTE since I run on the remote server (where it is hard to render this picture in a new window), so I have just savepng to achieve the same requirement\n",
    "fig.savefig(\"dataset.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.6807266354549091, w_module: 8.993435416843155\n",
      "iter: 10, loss: 0.0017987981535883216, w_module: 24.24719620628085\n",
      "iter: 20, loss: 0.0006559087075478755, w_module: 24.278731117586574\n",
      "iter: 30, loss: 0.00040468271995353306, w_module: 24.29791011862649\n",
      "iter: 40, loss: 0.00029533476581085355, w_module: 24.31222741327293\n",
      "iter: 50, loss: 0.00023502895577655027, w_module: 24.32390439532631\n",
      "iter: 60, loss: 0.0001975372302429116, w_module: 24.33393739688583\n",
      "iter: 70, loss: 0.00017254703748146867, w_module: 24.342868008663853\n",
      "iter: 80, loss: 0.00015515383557213166, w_module: 24.351025905003336\n",
      "iter: 90, loss: 0.00014270910670644287, w_module: 24.35862694836148\n",
      "iter: 100, loss: 0.00013364485862199925, w_module: 24.36581952662652\n",
      "iter: 110, loss: 0.00012696619563023196, w_module: 24.372709013530546\n",
      "iter: 120, loss: 0.00012200773772852687, w_module: 24.37937183936708\n",
      "iter: 130, loss: 0.00011830666516153261, w_module: 24.38586417165755\n",
      "iter: 140, loss: 0.00011553186653431897, w_module: 24.392227590096603\n",
      "iter: 150, loss: 0.00011344191992779994, w_module: 24.398492975547658\n",
      "iter: 160, loss: 0.00011185873074810935, w_module: 24.404683274144677\n",
      "iter: 170, loss: 0.00011065011301263176, w_module: 24.410815514075097\n",
      "iter: 180, loss: 0.00010971774770520899, w_module: 24.416902302412318\n",
      "iter: 190, loss: 0.00010898855183056591, w_module: 24.422952946699226\n",
      "iter: 200, loss: 0.00010840832985238076, w_module: 24.428974298611863\n",
      "iter: 210, loss: 0.00010793702800148566, w_module: 24.434971388509453\n",
      "iter: 220, loss: 0.00010754515790501497, w_module: 24.440947901445355\n",
      "iter: 230, loss: 0.00010721109544564976, w_module: 24.44690653278724\n",
      "iter: 240, loss: 0.00010691904409612988, w_module: 24.452849252624507\n",
      "iter: 250, loss: 0.00010665750541426628, w_module: 24.458777501383974\n",
      "iter: 260, loss: 0.00010641813636403063, w_module: 24.464692333858192\n",
      "iter: 270, loss: 0.00010619490042315994, w_module: 24.470594524781443\n",
      "iter: 280, loss: 0.00010598344045515553, w_module: 24.476484645913413\n",
      "iter: 290, loss: 0.00010578061784144687, w_module: 24.482363122126852\n",
      "iter: 300, loss: 0.00010558417541227285, w_module: 24.488230272100324\n",
      "iter: 310, loss: 0.00010539249196668067, w_module: 24.49408633777171\n",
      "iter: 320, loss: 0.00010520440415543326, w_module: 24.499931505617337\n",
      "iter: 330, loss: 0.0001050190776517321, w_module: 24.505765922004244\n",
      "iter: 340, loss: 0.0001048359142214864, w_module: 24.511589704256266\n",
      "iter: 350, loss: 0.00010465448483981712, w_module: 24.517402948626824\n",
      "iter: 360, loss: 0.00010447448164313811, w_module: 24.523205736042648\n",
      "iter: 370, loss: 0.00010429568346424782, w_module: 24.528998136243203\n",
      "iter: 380, loss: 0.00010411793114048563, w_module: 24.534780210766122\n",
      "iter: 390, loss: 0.00010394110984038516, w_module: 24.540552015102833\n",
      "iter: 400, loss: 0.00010376513642295909, w_module: 24.546313600257612\n",
      "iter: 410, loss: 0.00010358995040119237, w_module: 24.55206501387731\n",
      "iter: 420, loss: 0.0001034155074846507, w_module: 24.5578063010722\n",
      "iter: 430, loss: 0.00010324177496573989, w_module: 24.56353750501397\n",
      "iter: 440, loss: 0.00010306872842393227, w_module: 24.569258667373163\n",
      "iter: 450, loss: 0.00010289634937115669, w_module: 24.57496982864022\n",
      "iter: 460, loss: 0.00010272462356957196, w_module: 24.580671028362445\n",
      "iter: 470, loss: 0.00010255353982941791, w_module: 24.586362305319643\n",
      "iter: 480, loss: 0.00010238308914984534, w_module: 24.592043697655267\n",
      "iter: 490, loss: 0.00010221326410456038, w_module: 24.597715242974857\n",
      "iter: 500, loss: 0.00010204405840239808, w_module: 24.603376978420606\n",
      "iter: 510, loss: 0.00010187546657294358, w_module: 24.609028940728198\n",
      "iter: 520, loss: 0.00010170748374147231, w_module: 24.61467116627059\n",
      "iter: 530, loss: 0.00010154010546763256, w_module: 24.62030369109198\n",
      "iter: 540, loss: 0.0001013733276300716, w_module: 24.625926550934537\n",
      "iter: 550, loss: 0.00010120714634340409, w_module: 24.631539781259512\n",
      "iter: 560, loss: 0.00010104155789881934, w_module: 24.6371434172642\n",
      "iter: 570, loss: 0.00010087655872133285, w_module: 24.6427374938957\n",
      "iter: 580, loss: 0.00010071214533906898, w_module: 24.64832204586218\n",
      "iter: 590, loss: 0.00010054831436111418, w_module: 24.653897107642134\n",
      "iter: 600, loss: 0.00010038506246153678, w_module: 24.65946271349228\n",
      "iter: 610, loss: 0.00010022238636792132, w_module: 24.665018897454164\n",
      "iter: 620, loss: 0.00010006028285290021, w_module: 24.67056569335979\n",
      "iter: 630, loss: 9.989874872816863e-05, w_module: 24.67610313483656\n",
      "iter: 640, loss: 9.973778084000919e-05, w_module: 24.681631255311558\n",
      "iter: 650, loss: 9.957737606602129e-05, w_module: 24.68715008801534\n",
      "iter: 660, loss: 9.941753131275582e-05, w_module: 24.692659665985296\n",
      "iter: 670, loss: 9.925824351383616e-05, w_module: 24.698160022068723\n",
      "iter: 680, loss: 9.909950962871734e-05, w_module: 24.703651188925512\n",
      "iter: 690, loss: 9.894132664157101e-05, w_module: 24.709133199030724\n",
      "iter: 700, loss: 9.878369156046607e-05, w_module: 24.7146060846769\n",
      "iter: 710, loss: 9.862660141679594e-05, w_module: 24.72006987797618\n",
      "iter: 720, loss: 9.847005326472344e-05, w_module: 24.725524610862344\n",
      "iter: 730, loss: 9.83140441807139e-05, w_module: 24.73097031509271\n",
      "iter: 740, loss: 9.815857126325503e-05, w_module: 24.736407022249892\n",
      "iter: 750, loss: 9.800363163244518e-05, w_module: 24.741834763743515\n",
      "iter: 760, loss: 9.784922242978049e-05, w_module: 24.747253570811836\n",
      "iter: 770, loss: 9.76953408178567e-05, w_module: 24.7526634745233\n",
      "iter: 780, loss: 9.754198398013083e-05, w_module: 24.758064505778034\n",
      "iter: 790, loss: 9.738914912072883e-05, w_module: 24.763456695309284\n",
      "iter: 800, loss: 9.723683346416645e-05, w_module: 24.76884007368483\n",
      "iter: 810, loss: 9.708503425520971e-05, w_module: 24.77421467130836\n",
      "iter: 820, loss: 9.693374875866065e-05, w_module: 24.779580518420733\n",
      "iter: 830, loss: 9.678297425913152e-05, w_module: 24.784937645101365\n",
      "iter: 840, loss: 9.663270806088486e-05, w_module: 24.790286081269414\n",
      "iter: 850, loss: 9.648294748762827e-05, w_module: 24.795625856685078\n",
      "iter: 860, loss: 9.633368988235937e-05, w_module: 24.80095700095076\n",
      "iter: 870, loss: 9.618493260715204e-05, w_module: 24.80627954351231\n",
      "iter: 880, loss: 9.603667304304855e-05, w_module: 24.81159351366017\n",
      "iter: 890, loss: 9.58889085897694e-05, w_module: 24.816898940530557\n",
      "iter: 900, loss: 9.574163666566414e-05, w_module: 24.82219585310656\n",
      "iter: 910, loss: 9.559485470746236e-05, w_module: 24.827484280219334\n",
      "iter: 920, loss: 9.544856017013834e-05, w_module: 24.832764250549126\n",
      "iter: 930, loss: 9.530275052674393e-05, w_module: 24.83803579262645\n",
      "iter: 940, loss: 9.515742326820768e-05, w_module: 24.843298934833108\n",
      "iter: 950, loss: 9.501257590325568e-05, w_module: 24.848553705403305\n",
      "iter: 960, loss: 9.486820595817876e-05, w_module: 24.853800132424666\n",
      "iter: 970, loss: 9.47243109766936e-05, w_module: 24.859038243839322\n",
      "iter: 980, loss: 9.458088851979182e-05, w_module: 24.864268067444918\n",
      "iter: 990, loss: 9.443793616557634e-05, w_module: 24.869489630895643\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object \n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)\n",
    "\n",
    "# NOTE since I run on the remote server (where it is hard to render this picture in a new window), so I have just savepng to achieve the same requirement\n",
    "fig.savefig(\"training.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "train_prob, train_pred = LR.predict(x_train)\n",
    "# TODO: compute the accuracy\n",
    "# Convert predictions from {-1, 1} to {0, 1}\n",
    "# train_pred = np.where(train_pred == -1 , 0 , 1)\n",
    "train_acc = np.mean(train_pred == y_train)\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "test_prob , test_pred = LR.predict(x_test)\n",
    "# test_pred = np.where(test_pred == -1 , 0,1)\n",
    "test_acc = np.mean(test_pred == y_test)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.8338206391181163, w_module: 11.32850084425441\n",
      "iter: 10, loss: 1.3233735475057298, w_module: 17.17919315545479\n",
      "iter: 20, loss: 1.0682948510049262, w_module: 15.61201246864882\n",
      "iter: 30, loss: 0.887012881832086, w_module: 14.246365049809006\n",
      "iter: 40, loss: 0.7490010708006297, w_module: 13.086407315557901\n",
      "iter: 50, loss: 0.6445412909568738, w_module: 12.136250033768029\n",
      "iter: 60, loss: 0.5682888146742759, w_module: 11.393157726481482\n",
      "iter: 70, loss: 0.514926606943104, w_module: 10.841599358152878\n",
      "iter: 80, loss: 0.47910796544492884, w_module: 10.452937163736342\n",
      "iter: 90, loss: 0.4559130867931595, w_module: 10.191309999841135\n",
      "iter: 100, loss: 0.44129886444193367, w_module: 10.021462596463838\n",
      "iter: 110, loss: 0.4322639413302475, w_module: 9.914067060190572\n",
      "iter: 120, loss: 0.4267459202676459, w_module: 9.847373376301473\n",
      "iter: 130, loss: 0.4234006610241236, w_module: 9.806442507988578\n",
      "iter: 140, loss: 0.42138130716512595, w_module: 9.781511476891874\n",
      "iter: 150, loss: 0.42016523136698164, w_module: 9.766397929067352\n",
      "iter: 160, loss: 0.41943381430159127, w_module: 9.757263130927925\n",
      "iter: 170, loss: 0.4189941647758596, w_module: 9.751752318259456\n",
      "iter: 180, loss: 0.4187299603238965, w_module: 9.748431777230403\n",
      "iter: 190, loss: 0.4185711982976168, w_module: 9.74643258323174\n",
      "iter: 200, loss: 0.4184757947715666, w_module: 9.745229603546838\n",
      "iter: 210, loss: 0.4184184613806228, w_module: 9.744506036904548\n",
      "iter: 220, loss: 0.418384004425683, w_module: 9.744070978890512\n",
      "iter: 230, loss: 0.4183632951200748, w_module: 9.743809475328968\n",
      "iter: 240, loss: 0.4183508481707594, w_module: 9.743652340993687\n",
      "iter: 250, loss: 0.41834336722913334, w_module: 9.743557952264636\n",
      "iter: 260, loss: 0.41833887117528534, w_module: 9.743501274807077\n",
      "iter: 270, loss: 0.4183361692484049, w_module: 9.743467255824804\n",
      "iter: 280, loss: 0.41833454569531847, w_module: 9.743446846592075\n",
      "iter: 290, loss: 0.4183335702723877, w_module: 9.743434609057992\n",
      "iter: 300, loss: 0.41833298435797417, w_module: 9.74342727602571\n",
      "iter: 310, loss: 0.4183326324984394, w_module: 9.743422885178305\n",
      "iter: 320, loss: 0.418332421259009, w_module: 9.743420258351764\n",
      "iter: 330, loss: 0.41833229448665227, w_module: 9.743418688477892\n",
      "iter: 340, loss: 0.4183322184387094, w_module: 9.743417751419086\n",
      "iter: 350, loss: 0.4183321728425475, w_module: 9.7434171928982\n",
      "iter: 360, loss: 0.4183321455209786, w_module: 9.74341686057271\n",
      "iter: 370, loss: 0.4183321291614429, w_module: 9.74341666324183\n",
      "iter: 380, loss: 0.4183321193740509, w_module: 9.743416546357414\n",
      "iter: 390, loss: 0.4183321135244579, w_module: 9.743416477328703\n",
      "iter: 400, loss: 0.4183321100325253, w_module: 9.743416436708522\n",
      "iter: 410, loss: 0.418332107950958, w_module: 9.743416412910078\n",
      "iter: 420, loss: 0.4183321067122125, w_module: 9.74341639904223\n",
      "iter: 430, loss: 0.4183321059765159, w_module: 9.743416391015327\n",
      "iter: 440, loss: 0.4183321055406362, w_module: 9.743416386408514\n",
      "iter: 450, loss: 0.4183321052831396, w_module: 9.74341638379324\n",
      "iter: 460, loss: 0.41833210513155855, w_module: 9.743416382329665\n",
      "iter: 470, loss: 0.41833210504270946, w_module: 9.743416381526316\n",
      "iter: 480, loss: 0.41833210499090523, w_module: 9.743416381097223\n",
      "iter: 490, loss: 0.41833210496089857, w_module: 9.74341638087716\n",
      "iter: 500, loss: 0.41833210494366124, w_module: 9.743416380771519\n",
      "iter: 510, loss: 0.41833210493386325, w_module: 9.743416380726755\n",
      "iter: 520, loss: 0.4183321049283717, w_module: 9.743416380713022\n",
      "iter: 530, loss: 0.4183321049253508, w_module: 9.74341638071403\n",
      "iter: 540, loss: 0.41833210492373224, w_module: 9.743416380721149\n",
      "iter: 550, loss: 0.41833210492289813, w_module: 9.743416380730002\n",
      "iter: 560, loss: 0.418332104922494, w_module: 9.743416380738537\n",
      "iter: 570, loss: 0.41833210492232015, w_module: 9.743416380745929\n",
      "iter: 580, loss: 0.4183321049222641, w_module: 9.743416380751958\n",
      "iter: 590, loss: 0.41833210492226414, w_module: 9.743416380756697\n",
      "iter: 600, loss: 0.4183321049222878, w_module: 9.743416380760324\n",
      "iter: 610, loss: 0.41833210492231876, w_module: 9.743416380763056\n",
      "iter: 620, loss: 0.4183321049223498, w_module: 9.743416380765085\n",
      "iter: 630, loss: 0.4183321049223762, w_module: 9.743416380766575\n",
      "iter: 640, loss: 0.4183321049223982, w_module: 9.743416380767663\n",
      "iter: 650, loss: 0.4183321049224151, w_module: 9.74341638076845\n",
      "iter: 660, loss: 0.41833210492242817, w_module: 9.743416380769016\n",
      "iter: 670, loss: 0.4183321049224384, w_module: 9.743416380769421\n",
      "iter: 680, loss: 0.41833210492244577, w_module: 9.743416380769714\n",
      "iter: 690, loss: 0.41833210492245104, w_module: 9.74341638076992\n",
      "iter: 700, loss: 0.418332104922455, w_module: 9.743416380770068\n",
      "iter: 710, loss: 0.418332104922458, w_module: 9.743416380770173\n",
      "iter: 720, loss: 0.41833210492246004, w_module: 9.743416380770245\n",
      "iter: 730, loss: 0.4183321049224615, w_module: 9.743416380770297\n",
      "iter: 740, loss: 0.4183321049224625, w_module: 9.743416380770338\n",
      "iter: 750, loss: 0.41833210492246353, w_module: 9.743416380770366\n",
      "iter: 760, loss: 0.41833210492246403, w_module: 9.743416380770384\n",
      "iter: 770, loss: 0.4183321049224646, w_module: 9.743416380770398\n",
      "iter: 780, loss: 0.41833210492246453, w_module: 9.743416380770405\n",
      "iter: 790, loss: 0.4183321049224649, w_module: 9.743416380770414\n",
      "iter: 800, loss: 0.4183321049224649, w_module: 9.743416380770418\n",
      "iter: 810, loss: 0.41833210492246503, w_module: 9.743416380770418\n",
      "iter: 820, loss: 0.4183321049224649, w_module: 9.743416380770416\n",
      "iter: 830, loss: 0.4183321049224647, w_module: 9.743416380770418\n",
      "iter: 840, loss: 0.41833210492246514, w_module: 9.743416380770421\n",
      "iter: 850, loss: 0.4183321049224652, w_module: 9.743416380770421\n",
      "iter: 860, loss: 0.41833210492246514, w_module: 9.743416380770421\n",
      "iter: 870, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 880, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 890, loss: 0.4183321049224649, w_module: 9.743416380770421\n",
      "iter: 900, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 910, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 920, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 930, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 940, loss: 0.4183321049224649, w_module: 9.743416380770421\n",
      "iter: 950, loss: 0.41833210492246503, w_module: 9.743416380770421\n",
      "iter: 960, loss: 0.4183321049224649, w_module: 9.743416380770421\n",
      "iter: 970, loss: 0.4183321049224649, w_module: 9.743416380770421\n",
      "iter: 980, loss: 0.4183321049224649, w_module: 9.743416380770421\n",
      "iter: 990, loss: 0.4183321049224649, w_module: 9.743416380770421\n"
     ]
    }
   ],
   "source": [
    "# create a LogisticRegression object and train it when using regularization\n",
    "LR1 = LogisticRegression()\n",
    "\n",
    "# NOTE learning rate and reg are fancy hyperparameters to adjust.\n",
    "LR1.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)\n",
    "\n",
    "# NOTE since I run on the remote server (where it is hard to render this picture in a new window), so I have just savepng to achieve the same requirement\n",
    "fig.savefig(\"training_reg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "train_prob, train_pred = LR1.predict(x_train)\n",
    "\n",
    "# Convert predictions from {-1, 1} to {0, 1}\n",
    "# train_pred = np.where(train_pred == -1 , 0 , 1)\n",
    "train_acc = np.mean(train_pred == y_train)\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "test_prob , test_pred = LR1.predict(x_test)\n",
    "# test_pred = np.where(test_pred == -1 , 0,1)\n",
    "test_acc = np.mean(test_pred == y_test)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $||w||$ 的变化：不添加正则项时， $||w||$ 趋于一直增大；而添加正则化项后， $||w||$ 逐渐下降趋于平稳 \\\n",
    " \\\n",
    "正则化的实际意义：通过降低w的二阶范数大小降低w的复杂度，防止模型过拟合训练数据(模型可能过于复杂，过度拟合训练数据中的噪声或特定模式)。正则化项通过在损失函数中引入一个惩罚项，惩罚模型的复杂性，防止模型过度拟合训练数据，使模型倾向于选择较为简单的解，提升模型的泛化能力；正则化项的第二个目的是特征选择，（如L1正则化项）可以帮助去除不重要或冗余的特征（尤其对于大规模特征的数据集有效）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished at 12-23 @Boyuan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
