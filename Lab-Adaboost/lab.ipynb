{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "我们将使用以下数据集进行 Adaboost 的训练。\n",
    "\n",
    "该数据集与决策树部分使用的数据集相同，包括 7 个特征以及一个标签“是否适合攻读博士”，涵盖了适合攻读博士的各种条件，如love doing research,I absolutely want to be a college professor等。\n",
    "\n",
    "请执行下面的代码来加载数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read decision_tree_datasets.csv\n",
    "train_data = pd.read_csv('train_phd_data.csv')\n",
    "test_data = pd.read_csv('test_phd_data.csv')\n",
    "\n",
    "# translate lables [0,1] to [-1,1]\n",
    "# if 0 then -1, if 1 then 1\n",
    "train_data.iloc[:, -1] = train_data.iloc[:, -1].map({0: -1, 1: 1})\n",
    "test_data.iloc[:, -1] = test_data.iloc[:, -1].map({0: -1, 1: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost (15 pts)\n",
    "\n",
    "在上一个lab中，你已经成功完成了 Decision Tree 的构建。在本部分，你可以继续沿用上一部分的代码，学习并完成 Adaboost 模型的训练。\n",
    "\n",
    "在这个 Adaboost 模型中，我们选择了一层决策树作为弱学习器，并使用基尼系数作为分类标准。\n",
    "\n",
    "请完成以下类的构建以及相应函数的实现：\n",
    "\n",
    "1. **weakClassifier()**: 我们采用一层决策树，包括 `split()` 和 `predict()`。你可以参考上一次实验中的代码。\n",
    "\n",
    "2. **Adaboost()**：包括弱学习器的集合，拟合过程 `fit()` 和预测过程 `predict()`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weakClassifier:\n",
    "    def __init__(self):\n",
    "        \n",
    "\n",
    "        self.tree = None \n",
    "        self.alpha = None\n",
    "    \n",
    "    # here, we use the gini impurity to find the best feature and threshold\n",
    "    # Note: you need consider sample_weight when computing the gini impurity\n",
    "    \n",
    "    def best_split(self, X, y, sample_weight):\n",
    "\n",
    "        ''' \n",
    "            find the best feature and threshold to split the data based on the gini impurity\n",
    "\n",
    "            Args:\n",
    "                X: the features of the data\n",
    "                y: the labels of the data\n",
    "                sample_weight: the weight of each sample\n",
    "\n",
    "            Returns:\n",
    "                best_feature: the best feature to split the data\n",
    "                best_Series: Series, the data set after splitting\n",
    "        '''\n",
    "\n",
    "        # TODO: implement the function to find the best feature and threshold to split the data based on the gini impurity\n",
    "        best_feature = None\n",
    "        best_splits = None\n",
    "        best_gini = float('inf')\n",
    "\n",
    "        # Iterate over features and thresholds to find the best split\n",
    "        for feature in X.columns:\n",
    "            for threshold in X[feature].unique():\n",
    "                left_mask = X[feature] <=threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                left_labels = y[left_mask]\n",
    "                right_labels = y[right_mask]\n",
    "\n",
    "                left_gini = self.gini_impurity(left_labels,sample_weight[left_mask])\n",
    "                right_gini = self.gini_impurity(right_labels,sample_weight[right_mask])\n",
    "\n",
    "                current_gini = (len(left_labels) / len(y)) * left_gini + (len(right_labels) / len(y)) * right_gini\n",
    "                \n",
    "                #print(f\"debug for gini:{current_gini}\")\n",
    "\n",
    "                if current_gini < best_gini:\n",
    "                    best_gini = current_gini\n",
    "                    best_feature = feature\n",
    "                    best_splits = {\n",
    "                        'left': left_mask,\n",
    "                        'right': right_mask,\n",
    "                        'threshold':threshold\n",
    "                    }\n",
    "        return best_feature,best_splits\n",
    "    \n",
    "    def gini_impurity(self, labels, weights):\n",
    "        class_counts = labels.value_counts()\n",
    "        weights = np.array([weights[labels == label].sum() for label in class_counts.index])\n",
    "        gini = 1 - np.sum(((class_counts / len(labels)) ** 2)*weights)\n",
    "        return gini \n",
    "    \n",
    "    def fit(self, X, y, sample_weight,max_depth = 10):\n",
    "        '''  \n",
    "            fit the data to the decision tree\n",
    "\n",
    "            Args:\n",
    "                X: the features of the data\n",
    "                y: the labels of the data\n",
    "                sample_weight: the weight of each sample\n",
    "\n",
    "            Returns:\n",
    "                None, but self.tree should be updated\n",
    "        '''\n",
    "        best_feature, best_splits = self.best_split(X, y, sample_weight)\n",
    "\n",
    "        if best_feature is None or max_depth == 0:\n",
    "            return \n",
    "        self.tree = self.build_tree(X,y, sample_weight,best_feature, best_splits)\n",
    "    \n",
    "    def build_tree(self,X,y, sample_weight,best_feature, splits):\n",
    "        if len(set(y)) == 1:\n",
    "            return y.iloc[0]\n",
    "        \n",
    "        #print(f\"debug for splits:{splits}\")\n",
    "\n",
    "        if 'threshold' in splits:\n",
    "            best_threshold = splits['threshold']\n",
    "        else:\n",
    "            return y.mode().iloc[0]\n",
    "\n",
    "        decision_node ={\n",
    "            'feature':best_feature,\n",
    "            'threshold':best_threshold,\n",
    "            'left':None,\n",
    "            'right': None\n",
    "        }\n",
    "        left_mask = X[best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # print(f\"debug for left_mask:{left_mask} and right_mask:{right_mask}\")\n",
    "\n",
    "        if not left_mask.any() or not right_mask.any():\n",
    "        # If any mask is empty, it means there's no split needed\n",
    "            return y.mode().iloc[0]\n",
    "        decision_node['left'] = self.build_tree(X[left_mask], y[left_mask], sample_weight[left_mask], best_feature, splits['left'])\n",
    "        decision_node['right'] = self.build_tree(X[right_mask], y[right_mask], sample_weight[right_mask], best_feature, splits['right'])\n",
    "\n",
    "        return decision_node\n",
    "\n",
    "\n",
    "    def predict(self,x):\n",
    "        '''  \n",
    "        predict the label of the data\n",
    "\n",
    "        Args:\n",
    "            x: the features of the data\n",
    "        Return:\n",
    "            predict_lables: the predict labels of the data\n",
    "        '''\n",
    "\n",
    "        # Store the results\n",
    "        predict_lables = []\n",
    "\n",
    "        # predict the label of each sample\n",
    "        for i in range(len(x)):\n",
    "            sample = x.iloc[i,:]\n",
    "            node = self.tree\n",
    "            # TODO: predict the label of the sample\n",
    "            while isinstance(node,dict):\n",
    "                feature_value =sample[node['feature']]\n",
    "                if feature_value <= node['threshold']:\n",
    "                    node = node['left']\n",
    "                else :\n",
    "                    node = node['right']\n",
    "            predict_lables.append(node)\n",
    "        return predict_lables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    \n",
    "    def __init__(self, n_estimators=10):\n",
    "\n",
    "        # the number of weak classifier\n",
    "        self.n_estimators = n_estimators\n",
    "        # the list of weak classifier\n",
    "        self.clfs = []\n",
    "    \n",
    "    # AdaBoost training process\n",
    "    def fit(self, X, y):\n",
    "        n_samples,m_features = X.shape\n",
    "    \n",
    "        # initialize weights\n",
    "        w = np.ones(n_samples)/n_samples\n",
    "\n",
    "        # for each weak classifier\n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = weakClassifier()\n",
    "\n",
    "            # 1. fit the weak classifier\n",
    "            clf.fit(X,y,w)\n",
    "\n",
    "            # TODO: 2. predict the label of the data using the weak classifier\n",
    "            predict_labels = clf.predict(X)\n",
    "\n",
    "            # TODO: 3. Calculate errors \n",
    "            errors = (predict_labels !=y).astype(int)\n",
    "            error_rate = np.sum(errors * w) / np.sum(w)\n",
    "\n",
    "            # TODO:4. Calculate alpha\n",
    "            alpha = 0.5 *np.log((1-error_rate) / error_rate)\n",
    "            # TODO: 5. Update weights\n",
    "            w *= np.exp(-alpha *y *predict_labels)\n",
    "            # normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "\n",
    "            # save classifier and weight\n",
    "            clf.alpha = alpha\n",
    "            self.clfs.append(clf)\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        '''  \n",
    "        predict the label of the data\n",
    "        \n",
    "        Args:\n",
    "            X: the features of the data\n",
    "        Return:\n",
    "            y_pred: the predict labels of the data\n",
    "        '''\n",
    "\n",
    "        #TODO: 1. compute the predict labels of the data using all weak classifiers\n",
    "        predictions = np.array([clf.predict(X) for clf in self.clfs])\n",
    "\n",
    "        #TODO: 2. compute the weighted sum of the predict labels\n",
    "        \n",
    "        # NOTE: use np.newaxis to make predictions (num_classifiers, num_samples) and alpha values (num_classfiers, ) be compatible\n",
    "        weighted_sum = np.sum(predictions * np.array([clf.alpha for clf in self.clfs])[:,np.newaxis], axis=0)\n",
    "\n",
    "        #TODO: 3. get the label of the data by sign function (if x>0 return 1, else return -1)\n",
    "        return np.sign(weighted_sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1.]\n",
      "The accuracy of Adaboost is:  1.0\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = Adaboost(n_estimators=10)\n",
    "# fit the model\n",
    "adaboost_model.fit(train_data.iloc[:, :-1], train_data.iloc[:, -1])\n",
    "\n",
    "# TODO: predict the test data\n",
    "predictions = adaboost_model.predict(test_data.iloc[:,:-1])\n",
    "print(predictions)\n",
    "#print(test_data.iloc[:,-1])\n",
    "# TODO: calculate the accuracy of test data\n",
    "accuracy = np.mean(predictions == test_data.iloc[:,-1])\n",
    "print(\"The accuracy of Adaboost is: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished at 12-21 @Boyuan 2200017816"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
